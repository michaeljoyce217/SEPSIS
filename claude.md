# Sepsis Appeal Engine - Master Prompt

**Last Updated:** 2026-01-27
**Repo:** https://github.com/michaeljoyce217/SEPSIS

---

## Project Overview

**Goal:** Automated generation of DRG appeal letters for sepsis-related insurance denials (DRG 870/871/872).

**Architecture:** Three-file pipeline using Azure OpenAI GPT-4.1

**Platform:** Databricks on Azure with Unity Catalog

**Status:** POC Complete - Ready for production Epic workqueue integration

---

## Repository Structure

```
SEPSIS/
├── data/
│   ├── featurization_train.py        # ONE-TIME: Knowledge base ingestion (gold letters + propel)
│   └── featurization_inference.py    # PER-CASE: Data prep (denial + notes + structured data)
├── model/
│   └── inference.py                  # GENERATION: Vector search, write, assess, export
├── utils/
│   ├── gold_standard_appeals_sepsis_only/    # Current gold letters + default template
│   ├── gold_standard_appeals_sepsis_multiple/ # Future use
│   ├── sample_denial_letters/        # New denial letters to process (PDFs)
│   ├── propel_data/                  # Clinical criteria definitions (PDFs)
│   └── outputs/                      # Generated appeal letters (DOCX files)
├── docs/
│   ├── plans/                        # Design documents
│   ├── rebuttal-engine-overview.html # Technical overview (tabbed, detailed)
│   └── appeals-team-overview.html    # Simplified overview for appeals team
├── compare_denials.py                # Utility: check for duplicate denials
├── test_queries.sql                  # Validation queries for Unity Catalog
├── README.md                         # Project documentation
└── claude.md                         # This file (master prompt)
```

---

## Environment Note

**Catalog Access Pattern:** Data lives in the `prod` catalog, but we can only write to our current environment's catalog (`dev` or `prod`). This is intentional - all code uses `USE CATALOG prod;` for queries but writes tables with the `trgt_cat` prefix (e.g., `dev.fin_ds.fudgesicle_*`).

---

## Pipeline Architecture

### One-Time Setup: featurization_train.py
Run once to populate knowledge base tables:

| Step | Technology | Function |
|------|------------|----------|
| Gold Letter Parsing | Azure AI Document Intelligence + GPT-4.1 | Extract appeal/denial from gold PDFs |
| Denial Embedding | text-embedding-ada-002 | Generate 1536-dim vectors for similarity search |
| Propel Extraction | GPT-4.1 | Extract key clinical criteria from Propel PDFs |

### Per-Case Data Prep: featurization_inference.py
All data gathering for a single case. **Writes to case tables for inference.py to read.**

| Step | Technology | Function |
|------|------------|----------|
| 1. Parse Denial PDF | Azure AI Document Intelligence | OCR extraction from denial PDF |
| 2. Extract Denial Info | GPT-4.1 | Extract: account_id, payor, DRGs, is_sepsis |
| 3. Query Clinical Notes | Spark SQL | Get ALL notes from 14 types from Epic Clarity |
| 4. Extract Clinical Notes | GPT-4.1 | Extract SOFA components + clinical data with timestamps |
| 5. Query Structured Data | Spark SQL | Get labs, vitals, meds, diagnoses from Clarity |
| 6. Extract Structured Summary | GPT-4.1 | Summarize sepsis-relevant data with diagnosis descriptions |
| 7. Detect Conflicts | GPT-4.1 | Compare notes vs structured data for discrepancies |
| 8. Write Case Tables | Spark SQL | Write all outputs to case tables |

### Generation: inference.py
Reads prepared data from case tables (run featurization_inference.py first):

| Step | Technology | Function |
|------|------------|----------|
| 1. Load Case Data | Spark SQL | Read from case tables written by featurization_inference.py |
| 2. Vector Search | Cosine Similarity | Find best-matching gold letter (uses denial embedding) |
| 3. Letter Generation | GPT-4.1 | Generate appeal using gold letter + notes + structured data |
| 4. Strength Assessment | GPT-4.1 | Evaluate letter against Propel criteria, argument structure, evidence quality |
| 5. Export | python-docx | Output DOCX with assessment + conflicts appendix |

---

## Unity Catalog Tables

### Knowledge Base (populated by featurization_train.py)

| Table | Purpose |
|-------|---------|
| `fudgesicle_gold_letters` | Past winning appeals with denial embeddings |
| `fudgesicle_propel_data` | Official clinical criteria (definition_summary for prompts) |

### Case Data (populated by featurization_inference.py, read by inference.py)

| Table | Purpose |
|-------|---------|
| `fudgesicle_case_denial` | Denial text, embedding, payor, DRGs, is_sepsis flag |
| `fudgesicle_case_clinical` | Patient info + extracted clinical notes (JSON) |
| `fudgesicle_case_structured_summary` | LLM summary of structured data |
| `fudgesicle_case_conflicts` | Detected conflicts + recommendation |

### Intermediate Data (populated by featurization_inference.py)

| Table | Purpose |
|-------|---------|
| `fudgesicle_labs` | Lab results with timestamps |
| `fudgesicle_vitals` | Vital signs with timestamps |
| `fudgesicle_meds` | Medication administrations |
| `fudgesicle_diagnoses` | DX records with timestamps |
| `fudgesicle_structured_timeline` | Merged chronological timeline |

Note: All tables use the `{trgt_cat}.fin_ds.` prefix (e.g., `dev.fin_ds.fudgesicle_*`).

---

## Key Features

### Evidence Hierarchy
- **Primary Evidence:** Physician notes (clinical interpretation with medical judgment)
- **Supporting Evidence:** Structured data (objective lab values, vitals, medications)
- **Conflict Detection:** When structured data contradicts physician notes, flagged for CDI review

### 14 Clinical Note Types (from Epic Clarity)
Progress Notes, Consults, H&P, Discharge Summary, ED Notes, Initial Assessments, ED Triage Notes, ED Provider Notes, Addendum Note, Hospital Course, Subjective & Objective, Assessment & Plan Note, Nursing Note, Code Documentation

**Note:** ALL notes from the encounter are retrieved (not just most recent), concatenated chronologically with timestamps.

### SOFA Score Extraction
Note extraction prioritizes organ dysfunction data for quantifying sepsis severity:
- **Respiration:** PaO2/FiO2 ratio, oxygen requirements
- **Coagulation:** Platelet count
- **Liver:** Bilirubin
- **Cardiovascular:** MAP, vasopressor use with doses
- **CNS:** GCS (Glasgow Coma Scale)
- **Renal:** Creatinine, urine output
- **Plus:** Lactate trends, infection evidence, antibiotic timing

### Structured Data Summary
Labs, vitals, and medications are queried from Clarity and summarized by LLM for sepsis-relevant data:
- **Labs:** Lactate trends, WBC, procalcitonin, cultures, organ function (creatinine, bilirubin, platelets)
- **Vitals:** Temperature, MAP, heart rate, respiratory rate, SpO2, GCS
- **Meds:** Antibiotic timing (SEP-1 compliance), vasopressor initiation, fluid resuscitation

### Diagnosis Records (DX_NAME)
We query DX records directly from Epic's CLARITY_EDG table - these are more granular than ICD-10 codes:
- **DX_NAME** is the specific clinical description (e.g., "Severe sepsis with septic shock due to MRSA")
- **DX_ID** provides traceability to the source record
- All diagnoses include timestamps - LLM decides relevance based on date
- ICD-10 codes are NOT used - DX_NAME is what we quote in appeals

### Smart Note Extraction
Notes >8,000 chars are automatically extracted via LLM to pull relevant clinical data WITH timestamps (e.g., "03/15/2024 08:00: Lactate 4.2, MAP 63").

### Conflict Detection
Compares physician notes vs structured data to identify discrepancies:
- Note says "MAP maintained >65" but vitals show MAP <65
- Note says "lactate normalized" but labs show lactate still elevated
- Conflicts appear in DOCX appendix for CDI review

### Appeal Strength Assessment
After letter generation, an LLM evaluates the appeal and produces:
- **Overall score** (1-10) with LOW/MODERATE/HIGH rating
- **Summary** (2-3 sentences explaining the score)
- **Detailed breakdown** scoring three dimensions:
  - Propel Criteria Coverage (from: Propel definitions)
  - Argument Structure (from: denial letter, gold template)
  - Evidence Quality (from: clinical notes AND structured data)

Each finding is marked ✓ present, △ could strengthen, or ✗ missing. The "missing" items in Evidence Quality flag specific data points that weren't cited in the letter.

Assessment appears in DOCX before the letter body for CDI reviewer reference.

### Conservative DRG Extraction
Parser only extracts DRG codes if explicitly stated as numbers in the denial letter. Returns "Unknown" rather than hallucinating plausible codes.

### Markdown Bold Parsing
DOCX export converts markdown bold (`**text**`) to actual Word bold formatting for professional output.

### Propel Definition Summary
Full Propel PDFs are processed at ingestion time - LLM extracts key clinical criteria into `definition_summary` field for efficient prompt inclusion.

### Default Template Fallback
When vector search doesn't find a good match (score < 0.7), the system falls back to `default_sepsis_appeal_template.docx` as a structural guide. Score displays as "N/A" in output.

### Single-Letter Processing
Each denial is processed end-to-end in one run - no batch processing. This:
- Eliminates driver memory issues
- Matches production workflow (Epic workqueue feeds one case at a time)
- Simplifies debugging and testing

### Output Location
Appeal letters are saved to `utils/outputs/` with filename format: `{account_id}_{patient_name}_appeal.docx`

---

## Configuration

### featurization_train.py Flags
| Flag | Default | Description |
|------|---------|-------------|
| `RUN_GOLD_INGESTION` | False | Process gold standard letter PDFs |
| `RUN_PROPEL_INGESTION` | False | Process Propel definition PDFs |

### featurization_inference.py Settings
| Setting | Default | Description |
|---------|---------|-------------|
| `DENIAL_PDF_PATH` | (required) | Path to denial letter PDF to process |
| `KNOWN_ACCOUNT_ID` | None | Account ID (if known from Epic workqueue) |
| `NOTE_EXTRACTION_THRESHOLD` | 8000 | Char limit before LLM extraction |

### inference.py Settings
| Setting | Default | Description |
|---------|---------|-------------|
| `MATCH_SCORE_THRESHOLD` | 0.7 | Minimum similarity for gold letter match |
| `EXPORT_TO_DOCX` | True | Export as Word documents |

---

## Cost Estimates

Based on Azure OpenAI GPT-4.1 standard pricing ($2.20/1M input, $8.80/1M output):

### Per Appeal Letter (~$0.30)
| Step | Input Tokens | Output Tokens | Cost |
|------|-------------|---------------|------|
| Denial info extraction | ~4,000 | ~100 | $0.01 |
| Note extraction (4 calls avg) | ~12,000 | ~3,200 | $0.05 |
| Structured data extraction | ~8,000 | ~1,500 | $0.03 |
| Conflict detection | ~6,000 | ~500 | $0.02 |
| Appeal letter generation | ~55,000 | ~3,000 | $0.15 |
| Strength assessment | ~15,000 | ~800 | $0.04 |
| **Total** | ~100,000 | ~9,100 | **~$0.30** |

### Monthly Projections
| Volume | LLM Cost |
|--------|----------|
| 100 appeals/month | ~$30 |
| 500 appeals/month | ~$150 |
| 1,000 appeals/month | ~$300 |

**One-time setup:** <$1 for gold letter + Propel ingestion

---

### POC vs Production

**POC Mode:** Set `KNOWN_ACCOUNT_ID = None` - LLM extracts account ID from denial letter text. Some generic denials may lack identifiable information.

**Production Mode:** Set `KNOWN_ACCOUNT_ID = "12345678"` - Epic workqueue provides account ID directly, enabling 100% coverage.

---

## Quick Start (Databricks)

1. **Install dependencies** (run Cell 1 alone, then restart):
   ```python
   %pip install azure-ai-documentintelligence==1.0.2 openai python-docx
   dbutils.library.restartPython()
   ```

2. **One-time setup** - Run `featurization_train.py` with flags enabled:
   ```python
   RUN_GOLD_INGESTION = True   # First run
   RUN_PROPEL_INGESTION = True # First run
   ```

3. **Prepare case data** - In `featurization_inference.py`, set the PDF path:
   ```python
   DENIAL_PDF_PATH = "/path/to/denial.pdf"
   ```
   Run the notebook to prepare all case data (writes to case tables).

4. **Generate appeal** - Run `inference.py`:
   - Reads prepared case data from tables
   - Generates appeal letter
   - Outputs DOCX to `utils/outputs/`

5. **Review output** - includes assessment section and conflicts appendix (if any)

---

## Technology Stack

| Component | Technology |
|-----------|------------|
| Platform | Databricks on Azure |
| LLM | Azure OpenAI GPT-4.1 |
| Embeddings | text-embedding-ada-002 (1536 dims, 30k char limit) |
| Document OCR | Azure AI Document Intelligence |
| Storage | Delta Lake tables in Unity Catalog |
| Clinical Data | Epic Clarity |

---

## Team

**Financial Data Science** | Mercy Hospital
